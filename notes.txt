---------------------------------------
PLAN / TODO
---------------------------------------
- compile and run the storage server using docker - right now, just deploy one server
- error handling / more comprehensive testing / testing of edge cases
- MAYBE: make MasterServer and StorageServer inherit from a more general HttpServer
- think about adding / removing nodes on the fly (as in - don't have to stop the master to do this)
    - potentially requires nodes communicating with one another
    - OR, when you transfer data, just do it in one big fuck off load
- think about replication

general todo:
    - change prettyPrint to toString() that actually returns a string
    - (as much as possible) change tests from simple prints to actual 'red-and-green' tests
        - create c++ testing module (perhaps re-use one from jpeg compression project (think I had one in there))
          to print red and green test text easily
        - consider: right now, all my std::cout is printing alongside test messages
            - want my test messages to appear in a separate stream, so its nice and clear
              which are passing and which aren't
    - generally error check better
        - change 'print-returns' to error throws
    - generally clean up

toLearn:
    - exactly when unique_ptr / shared_ptr goes out of scope
      and generally when things get auto de-allocated
    
---------------------------------------
SCRIBBLES
---------------------------------------
Handling duplicates KEYs (i.e. when PUT gives key that's already used):
    - for now, automatically overrides
    - rationale:
        - want a ligthweight API that can be easily accessed via curl
        - therefore, 'are you sure?' messages upon overwritting is not 
          really what we want
        - can have a GET endpoint /keys that gives you a list of keys
          in use and their files sizes / num blocks used

---------------------------------------
IMPLEMENTATION DETAILS
---------------------------------------

Master
------
- exposes GET/PUT/DELETE API
- breaks files into blocks and distributes across cluster of storage nodes
- uses consistent hashing to distribute blocks evenly

KBN:
    - master stores KEY -> {blockNum -> nodeNum} mapping
    - call this the KeyBlockNode map, or the KBN

HashRing:
    - nodes evenly distributed around ring of (2^32 - 1) values
    - use virtual nodes to ensure even distribution in 'low-node' environment
        - each physical node has N virtual nodes
        - blocks are assigned to a virtual node, which implictly assigns
          it to a physical node
    - flow:
        - add block (PUT):
            - compute hash(key + blockNum)
            - find nearest virtual node, which stores its physical node id
            - lookup in physical node table to find IP
        - add node:
            - 
        - delete node:
            -

API (client -> master):
    - endpoint: /store/{KEY}
    PUT:
        - receives (KEY, payload)
        - create entry in the KBN
        - breaks up payload into {block1, block2, ...}
        - for each block:
            - use consistent hashing (key+blockNum) to determine 
            which node stored on
            - send PUT to relevant node with (KEY, blockNum, blockPayload)
            - on success, add (blockNum->nodeNum) entry to KBN

    GET:
        - receives (KEY) to GET
        - lookup KEY entry in KBN
        - for each block:
            - send GET to relevant node with (KEY, blockNum)
            - add (in order) to a buffer
        - package up buffer into response payload

    DELETE:
        - receives (KEY) to DELETE
        - lookup KEY entry in KBN
        - for each block:
            - send DELETE to relevant node with (KEY, blockNum)

Other responsibilities:
    Adding/removing nodes
        - options:
            - manually (i.e. config file of ip:port addresses)
            - dynamically:
                - expose an API to add / remove nodes
                - only let authorised users make these calls

        - Adding:

        - Removing:
            - when remove a node, must re-assign all its blocks
              as per the consistent hashing strategy
    
    
    Checking health of nodes:
        - heartbeat method
        - send GET req to specialised heartbeat addr

Cluster
-------
- each cluster node just treated as a bucket of free/occupied blocks
- on disk format:
    Header:
        Magic Number - 4 bytes
        BAT offset - 4 bytes
        Block size - 4 bytes
    BAT:
        TBD
    Block 0
    Block 1
    ...
    Block N
API (master -> cluster node)
    GET:
        - receives (KEY, blockNum)
        - looks up offset in BAT
        - returns block

    PUT:
        - receives (KEY, blockNum, blockPayLoad)
        - inserts new BAT entry: (KEY, blockNum) -> offset
        - writes block at offset

    DELETE:
        - receives (KEY, blockNum)
        - removes BAT entry

Re-assignment:
    - when node added/deleted, blocks are re-assigned
    - add M:
        - Have L, N  -> L, M, N
        - re-assign some of N's blocks to M
    - remove M:
        - Have L, M, N
        - re-assign all M's blocks to N
    - re-assigning:
        - already have `old` block->node assignments
        - using `old` block->node assignments, figure out which
          blocks need to be moved
    - problem:
        - say key1 gets divided into block chunks [1,2,3] (across 3 nodes)
        - on each node, chunk of blocks are stored contiguously
        - when we re-assign, some blocks in that chunk will be moved
            